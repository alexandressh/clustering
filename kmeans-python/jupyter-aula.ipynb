{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bag of words - vectorizer.fit_transform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import re\n",
    "\n",
    "from time import time\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "url = 'data-pre-processing.csv'\n",
    "dataframe = pandas.read_csv(url)\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "stemmer = PorterStemmer()\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "corpus = []\n",
    "corpusClassification = []\n",
    "\n",
    "# print(\"Removing contraction - replacer.replace\")\n",
    "# print(\"Removing special chars - re.sub\") -> Pay attention to <br> </br> (ouseébrHEUehUE)\n",
    "# print(\"Steeming words - steemer.stem\")\n",
    "for videoId,author,date,content,classification in dataframe.values:\n",
    "    comment = []\n",
    "    content = content.lower()\n",
    "    content = re.sub('(\\s|\\<)br\\s*.\\>', '', content)\n",
    "    content = re.sub('(href.*)?http(.*)(\\.com)?', 'HTTPWEBSITE', content)\n",
    "    content = re.sub('[^A-Za-z0-9\\s]+', '', content)\n",
    "    corpusClassification.append(classification)\n",
    "    for word in tokenizer.tokenize(content):\n",
    "            word = stemmer.stem(word)\n",
    "            comment.append(word)\n",
    "    corpus.append(\" \".join(comment))\n",
    "\n",
    "print(\"Creating bag of words - vectorizer.fit_transform\")\n",
    "print()\n",
    "\n",
    "vectorized =  vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting K-means\n",
      "0.0128636375328\n",
      "Top terms per cluster:\n",
      "Cluster 0: httpwebsit song love thi subscrib like check channel pleas video\n",
      "Cluster 1: youtub check video thi playlist look tri like httpwebsit pleas\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "silhouetteValues = []\n",
    "maxK = 4\n",
    "\n",
    "print(\"Starting K-means\")\n",
    "# for k in range(2, maxK):\n",
    "#     km = KMeans(n_clusters=k, init='k-means++', max_iter=1000, verbose=False)\n",
    "#     t0 = time()\n",
    "#     km.fit(vectorized.toarray())\n",
    "#     labels = km.labels_\n",
    "#     silhouette = metrics.silhouette_score(vectorized.toarray(), labels, metric='euclidean')\n",
    "#     silhouetteValues.append(silhouette)\n",
    "#     print(\"Clusters: %d - %0.5f - done in %0.3fs\" % (k, silhouette, (time() - t0)))\n",
    "#     true_k = k\n",
    "\n",
    "# plt.title('Silhouette')\n",
    "# plt.plot(list(range(2,maxK)), silhouetteValues)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "true_k = 2\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=5000, verbose=False)\n",
    "km.fit(vectorized.toarray())\n",
    "labels = km.labels_\n",
    "# result = km.predict(vectorized.toarray())\n",
    "rand = adjusted_rand_score(corpusClassification, labels)\n",
    "print(rand)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute structured hierarchical clustering...\n",
      "done in 8.970s\n",
      "\n",
      "Top terms per cluster:\n",
      "[[ 134  159]\n",
      " [ 585 1956]\n",
      " [ 825 1957]\n",
      " ..., \n",
      " [2107 3801]\n",
      " [3882 3907]\n",
      " [3908 3909]]\n",
      "[0 0 0 ..., 0 8 8]\n",
      "Cluster 0: 500 753\n",
      "Cluster 1: chack life\n",
      "Cluster 2: datpiffcommixtapesdetailphpid633807 life39\n",
      "Cluster 3: dispos lift\n",
      "Cluster 4: frigea light\n",
      "Cluster 5: hate lik\n",
      "Cluster 6: hrefhttpssoundcloudcomroccsteadywaveemojiprodbynippylongbottomcyberpunkhttpssoundcloudcomroccsteadywaveemojiprodbynippylongbottomcyberpunka like\n",
      "Cluster 7: httpminhatecacombrmaurosp2013filmesseriesdesenhosanimesmp3etc likeamp\n",
      "Cluster 8: httpshhortcomaroocnjqu2b likebr\n",
      "Cluster 9: jaylan likecom\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "\n",
    "n_clusters = 10  # number of regions\n",
    "ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "\n",
    "t0 = time()\n",
    "ward.fit(vectorized.toarray())\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print()\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = ward.children_\n",
    "terms = vectorizer.get_feature_names()\n",
    "print(ward.children_)\n",
    "print(ward.labels_ )\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z13rv5wie br u2fdnogl04cdl0bdmjuunpwcnk0k,C Williams,2015-05-18T21:51:36.079000,Check out this playlist on YouTubeï»¿,1\n"
     ]
    }
   ],
   "source": [
    "comment = \"z13rv5wie br u2fdnogl04cdl0bdmjuunpwcnk0k,C Williams,2015-05-18T21:51:36.079000,Check out this playlist on YouTube<br />ï»¿,1\"\n",
    "\n",
    "comment = re.sub('(\\s|\\<)br\\s*.\\>', '', comment)\n",
    "\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
